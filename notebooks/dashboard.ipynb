{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71023875",
   "metadata": {},
   "source": [
    "# nr-learn Dashboard\n",
    "\n",
    "このNotebookは `train -> predict -> backtest` の成果物をまとめて確認するためのダッシュボードです。\n",
    "\n",
    "## 前提ファイル\n",
    "- `artifacts/reports/train_metrics.json`\n",
    "- `artifacts/predictions/predictions_*.csv`\n",
    "- `artifacts/reports/backtest_*.json`\n",
    "\n",
    "最初に学習・予測・バックテストを実行してから開いてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a526bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "ARTIFACTS = ROOT / \"artifacts\"\n",
    "PRED_DIR = ARTIFACTS / \"predictions\"\n",
    "REPORT_DIR = ARTIFACTS / \"reports\"\n",
    "\n",
    "def latest_file(path: Path, pattern: str) -> Path:\n",
    "    files = sorted(path.glob(pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files matched: {path}/{pattern}\")\n",
    "    return files[-1]\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"PRED_DIR:\", PRED_DIR)\n",
    "print(\"REPORT_DIR:\", REPORT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd75e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics_path = REPORT_DIR / \"train_metrics.json\"\n",
    "if not train_metrics_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {train_metrics_path}\")\n",
    "\n",
    "with train_metrics_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    train_metrics = json.load(f)\n",
    "\n",
    "pd.DataFrame([train_metrics]).T.rename(columns={0: \"value\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8104d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = latest_file(PRED_DIR, \"predictions_*.csv\")\n",
    "backtest_path = latest_file(REPORT_DIR, \"backtest_*.json\")\n",
    "\n",
    "pred_df = pd.read_csv(pred_path)\n",
    "with backtest_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    backtest = json.load(f)\n",
    "\n",
    "summary = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"prediction_file\": pred_path.name,\n",
    "            \"rows\": len(pred_df),\n",
    "            \"races\": pred_df[\"race_id\"].nunique() if \"race_id\" in pred_df.columns else None,\n",
    "            \"top1_hit_rate\": backtest.get(\"top1_hit_rate\"),\n",
    "            \"top3_hit_rate\": backtest.get(\"top3_hit_rate\"),\n",
    "            \"top5_hit_rate\": backtest.get(\"top5_hit_rate\"),\n",
    "            \"simple_top1_win_roi\": backtest.get(\"simple_top1_win_roi\"),\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4.5))\n",
    "\n",
    "axes[0].hist(pred_df[\"score\"], bins=25, color=\"#3b82f6\", alpha=0.85)\n",
    "axes[0].set_title(\"Prediction Score Distribution\")\n",
    "axes[0].set_xlabel(\"score\")\n",
    "axes[0].set_ylabel(\"count\")\n",
    "\n",
    "if \"race_id\" in pred_df.columns:\n",
    "    race_top = pred_df.groupby(\"race_id\", as_index=False)[\"score\"].max()\n",
    "    axes[1].boxplot(race_top[\"score\"], vert=True)\n",
    "    axes[1].set_title(\"Per-race Top Score Boxplot\")\n",
    "    axes[1].set_ylabel(\"top score\")\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, \"race_id not found\", ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1116414",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"rank\" in pred_df.columns:\n",
    "    calib = pred_df.copy()\n",
    "    calib[\"is_win\"] = (calib[\"rank\"] == 1).astype(int)\n",
    "    calib[\"score_bin\"] = pd.cut(calib[\"score\"], bins=10, include_lowest=True)\n",
    "    calib_summary = calib.groupby(\"score_bin\", observed=False).agg(\n",
    "        mean_score=(\"score\", \"mean\"),\n",
    "        win_rate=(\"is_win\", \"mean\"),\n",
    "        count=(\"is_win\", \"size\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(calib_summary[\"mean_score\"], calib_summary[\"win_rate\"], marker=\"o\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.title(\"Calibration-style Plot\")\n",
    "    plt.xlabel(\"mean predicted score\")\n",
    "    plt.ylabel(\"actual win rate\")\n",
    "    plt.grid(alpha=0.25)\n",
    "    plt.show()\n",
    "\n",
    "    calib_summary\n",
    "else:\n",
    "    print(\"rank 列がないため、Calibrationプロットはスキップ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0908df",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [c for c in [\"race_id\", \"horse_id\", \"horse_name\", \"score\", \"pred_rank\", \"rank\"] if c in pred_df.columns]\n",
    "(\n",
    "    pred_df[columns]\n",
    "    .sort_values(\"score\", ascending=False)\n",
    "    .head(20)\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd508f",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- `run_predict` を複数日ループで回して、`predictions_*.csv` を日次で蓄積する\n",
    "- `run_backtest` で日次JSONを作り、時系列のTop1/Top3推移を比較する\n",
    "- 特徴量を追加（騎手・調教師の期間別成績、コース×距離相性）して再学習する\n",
    "- モデル比較（LightGBM vs RandomForest）を同じ分割条件で行う"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
